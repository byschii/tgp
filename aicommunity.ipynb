{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mettere Watermark a testi generati da (Large) Language Models\n",
    "Ma prima, una minima introduzione ad un concetto di base.\n",
    "\n",
    "## L'output di GPT\n",
    "La prendo leggermente alla larga.\n",
    "\n",
    "Cos'è GPT? L'\"[architettura](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\" su cui si basano i famosi GPT-qualcosa, tipo [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). (ho messo due link ai paper, ma più che altro per le immagini)\n",
    "\n",
    "**Cosa c'è di importante da sapere di questa architettura?** Ciò che prende in input e ciò che restituisce in output.\n",
    "\n",
    "Forse, la parte di input, è banale: viene alimentata con una lista di parole (che è una semplificazione, ma non si perde troppo del significato per quello che cercherò di spiegare).\n",
    "\n",
    "Ma è importante complicare leggermente la descrizione dell'output. **GPT restituisce, per ogni parola che \"conosce\", la probabilità che questa sia quella che meglio prosegue la frase che ha ricevuto in input.** I parametri `temperature` e `top_p`, servono ad influenzare come le parole vengono scelte, ma non serve capire cosa fanno.\n",
    "\n",
    "![per rendere l'idea](./assets/output-gpt.png)\n",
    "\n",
    "## Ora un piccolo esempio di generazione (senza watermark)\n",
    "\n",
    "Prima di tutto, serve che \"tiro fuori\" il mio mini modello, dando la posizone di iperparametri e pesi.\n",
    "\n",
    "Per una serie di ragioni (mi diverte, volevo esempi italiani, non volevo legarmi ad Hugging Face) ho addestrato io un piccolo GPT (0.1*GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out-model' # cartella dalla quale caricare il modello\n",
    "ckpt_file='ckpt3450.pt' # ultimi pesi+iperparametri del modello\n",
    "tokenizer_folder='tokenizer' # cartella contenente il tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poi si inizializzano alcune cose di torch e il modello GPT vero e proprio.\n",
    "\n",
    "> Io mi sono schierato dal lato oscuro, quello di [Torch](https://pytorch.org/), ma rifare in Keras non dovrebbe essere troppo complicato.\n",
    "\n",
    "Colgo l'occasione anche per una brave menzione d'onore al buon **[Andrej Karpathy](https://karpathy.ai/)**, che ha fatto un fantastico \"corso\" su [Youtube](https://youtu.be/kCc8FmEb1nY) dove spiega in maniera ✨meravigliosa✨ come funziona GPT, entrando molto chiaramente anche nel meccanismo dell'attenzione. Molto gentilmente ha messo su [Github il codice](https://github.com/karpathy/nanoGPT) del \"suo\" GPT e *io l'ho rubato*.\n",
    "\n",
    "Secondo me è un ottimo punto per partire, o anche solo per incominciare a costruire un intuizione di come questi modelli funzionano.\n",
    "\n",
    "Per brevità non scendo nei dettagli, al momento il punto e generare del testo d'esempio, senza watermark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 33.61M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from model import GPTConfig, GPT\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import numpy as np\n",
    "\n",
    "seed = random.randint(1,9999) # un numero casuale\n",
    "device = 'cpu' # per semplicita\n",
    "dtype = 'bfloat16' # per device\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# istanzio il modello\n",
    "checkpoint = torch.load(os.path.join(out_dir, ckpt_file), map_location=device)\n",
    "model = GPT(GPTConfig(**checkpoint['model_args']))\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# istanzio il tokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(f\"{tokenizer_folder}/ttookk-vocab.json\", f\"{tokenizer_folder}/ttookk-merges.txt\")\n",
    "encode = lambda s: np.array(tokenizer.encode(s).ids)\n",
    "decode = lambda l: tokenizer.decode(l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variabili e funzioni per generazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "max_new_tokens = 30 # quante \"\"parole\"\" generare\n",
    "temperature = 0.65\n",
    "top_k = 70\n",
    "initial_prompt = \"per simulare\"\n",
    "\n",
    "def encode_input(text):\n",
    "    tokenized_text = encode(text)\n",
    "    return torch.tensor(tokenized_text, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_logits(gpt_model, tokenized_input, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    funzione per generare gli score associati ad ogni token\n",
    "    \n",
    "    temperature: float, piu alto piu casuale\n",
    "    top_k: int, taglia la coda delle probabilità piu basse\n",
    "    \"\"\"\n",
    "    # taglio l input se non sta nel contesto\n",
    "    ctx_len = gpt_model.config.block_size \n",
    "    idx_cond = tokenized_input if tokenized_input.size(1) <= ctx_len else tokenized_input[:, -ctx_len:]\n",
    "    \n",
    "    # eseguo il modello\n",
    "    logits, _ = gpt_model(idx_cond)\n",
    "    \n",
    "    # applico temperature e top_k alle probabilità\n",
    "    logits = logits[:, -1, :] / temperature\n",
    "    # optionally crop the logits to only the top k options\n",
    "    if top_k is not None:\n",
    "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora che ci sono tutti gli ingredienti di base, si può effettivamente provare a generare qualcosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: per simulare\n",
      "generated text: per simulare la propria capacità di generare un'economia in larga scala. L'economia del Paese ha un carattere di ricerca e di sviluppo sostenibile. Il governo Popolare\n",
      "[549, 11705, 607, 298, 1791, 2524, 271, 10789, 296, 6, 4118, 286, 5258, 4031, 13, 347, 6, 4118, 279, 3991, 536, 296, 3513, 271, 1834, 277, 271, 1748, 15996, 13, 508, 1316, 8446]\n"
     ]
    }
   ],
   "source": [
    "x_not_watermarked = encode_input(initial_prompt)\n",
    "for _ in range(max_new_tokens):\n",
    "    logits = get_logits(model, x_not_watermarked, temperature, top_k)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    x_not_watermarked = torch.cat((x_not_watermarked, idx_next), dim=1)\n",
    "\n",
    "print(\"input text:\", initial_prompt)\n",
    "print(\"generated text:\", decode(x_not_watermarked[0].tolist()))\n",
    "print(x_not_watermarked[0].tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho stampato una serie di numeri, perché all inizio ho semplificato e il codice mi costiringe a spiegare qualcosaltro.\n",
    "\n",
    "In realtà, tra le string in input e il nostro GPT, c'è un altro pezzo, si chiama Tokenizer ed ha il compito di prendere la stringa tipo \"mondo\" e trasformarla in una lista di numeri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per simulare ->  [  549 11705   607]\n",
      "549 -> per\n",
      "11705 ->  simu\n",
      "607 -> lare\n"
     ]
    }
   ],
   "source": [
    "print(f\"{initial_prompt} -> \", encode(initial_prompt))\n",
    "\n",
    "encoded = encode(initial_prompt)\n",
    "for i in range(len(encoded)):\n",
    "    print(f\"{encoded[i]} -> {decode([encoded[i]])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per applicare il nostro watermark, ci inseriremo al livello delle probabilità che il modello assegna ad ogni token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: per simulare\n",
      "tok \t   dec \t   punteg \t   prob\n",
      "298 \t |  la \t | 13.05 \t | 26.32%\n",
      "305 \t |  il \t | 12.72 \t | 18.86%\n",
      "325 \t |  l \t | 12.28 \t | 12.21%\n",
      "350 \t |  le \t | 12.08 \t | 10.01%\n",
      "281 \t |  i \t | 12.00 \t | 9.21%\n",
      "296 \t |  un \t | 11.98 \t | 9.01%\n",
      "376 \t |  una \t | 11.40 \t | 5.04%\n",
      "507 \t |  gli \t | 10.26 \t | 1.62%\n",
      "11 \t | , \t | 10.02 \t | 1.27%\n",
      "277 \t |  e \t | 9.59 \t | 0.83%\n",
      "448 \t |  lo \t | 9.33 \t | 0.64%\n",
      "661 \t |  questo \t | 8.90 \t | 0.41%\n",
      "1377 \t |  immagini \t | 8.75 \t | 0.36%\n",
      "499 \t |  anche \t | 8.57 \t | 0.3%\n",
      "286 \t |  in \t | 8.57 \t | 0.3%\n"
     ]
    }
   ],
   "source": [
    "print_top_k = 15\n",
    "\n",
    "def print_next_token_stats(esempio):\n",
    "    x = encode_input(esempio)\n",
    "    logits = get_logits(model, x, temperature, top_k)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # prints the logits of 10 most probable tokens\n",
    "    logits_x_token = logits[0].topk(print_top_k).values.tolist()\n",
    "    tokens = logits[0].topk(print_top_k).indices.tolist()\n",
    "    probs_x_token = probs[0].topk(print_top_k).values.tolist()\n",
    "\n",
    "    print(f\"input text: {esempio}\")\n",
    "    print(\"tok \\t   dec \\t   punteg \\t   prob\")\n",
    "    for i in range(len(tokens)):\n",
    "        ti = tokens[i]\n",
    "        dti = decode([ti])\n",
    "        lti = logits_x_token[i]\n",
    "        pti = round(probs_x_token[i]*100, 2)\n",
    "        print(f\"{ti} \\t | {dti} \\t | {lti:.2f} \\t | {pti}%\")\n",
    "\n",
    "print_next_token_stats(initial_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK ma come si fa il watermark?\n",
    "\n",
    "Tornando all'azzurrissimo esempio di cui sopra, l'idea è quella di manipolare le probabilità (o meglio, i \"punteggi\" prima che vengano trasformati in probabilità).\n",
    "\n",
    "Per farlo, ciò che viene suggerito da [alcuni più svegli di me](https://arxiv.org/abs/2301.10226) è di creare una algoritmo che a partire dall'input, genera una maschera, in corrispondenza della quale si modifichino i punteggi associati ai vari token nel vocabolario del GPT.\n",
    "\n",
    "![masking](./assets/masking.png)\n",
    "\n",
    "Questo processo è da ripetere per ogni parola.\n",
    "\n",
    "Siccome noi conosciamo e possiamo ripetere l'algoritmo che genera la maschera, potremo in futuro controllare se un qualsiasi input è stato generato che il LLM al quale abbiamo applicato il nostro watermark.\n",
    "\n",
    "### L'algoritmo piu semplice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatermarkBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: list[int] = None,\n",
    "        gamma: float = 0.5, # quanto del vocabolario alterare\n",
    "        delta: float = 2, # quanto perturberemo la probabilità\n",
    "        hash_key: int = 15485863,  # deve solo essere un numero primo grandino\n",
    "    ):\n",
    "\n",
    "        # watermarking parameters\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.device = 'cpu'\n",
    "        self.rng = torch.Generator(device=self.device)\n",
    "        self.hash_key = hash_key\n",
    "\n",
    "    def _seed_rng(self, prev_token: int) -> None:\n",
    "        # per inizilaizzare il generatore di numeri casuali\n",
    "        self.rng.manual_seed(self.hash_key * prev_token)\n",
    "\n",
    "    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\n",
    "        # inizializzo il generatore di numeri casuali\n",
    "        # prendo l'ultimo token\n",
    "        calc_seed = lambda x: x[-1].item() if type(x) == torch.Tensor else x[-1]\n",
    "        self._seed_rng(calc_seed(input_ids))\n",
    "\n",
    "        # mescolo il vocabolario\n",
    "        vocab_permutation = torch.randperm(self.vocab_size, device=self.device, generator=self.rng)\n",
    "        # decido dove tagliare\n",
    "        greenlist_size = int(self.gamma * self.vocab_size)\n",
    "        # prendo i primi\n",
    "        # avrò una lista casuale di id del vocabolario, la maschera\n",
    "        greenlist_ids = vocab_permutation[:greenlist_size]\n",
    "        return greenlist_ids   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta preparato l'algoritmo di base per la generazione degli id da alterare, serve qualcosa per **applicare** il watermark e qualcosa per **riconoscerlo** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMWatermarker(WatermarkBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _calc_greenlist_mask(self, scores: torch.FloatTensor, green_token_ids) -> torch.BoolTensor:\n",
    "        \"\"\"\n",
    "        crea un vettore come il vocabolario\n",
    "        con 1 dove ci sono i token da pompare\n",
    "        \"\"\"\n",
    "        green_tokens_mask = torch.zeros_like(scores)\n",
    "        green_tokens_mask[0][green_token_ids] = 1\n",
    "        final_mask = green_tokens_mask.bool()\n",
    "        return final_mask\n",
    "\n",
    "    def watermark(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        seed_token = input_ids[-1]\n",
    "        greenlist_ids = self._get_greenlist_ids(seed_token)\n",
    "        green_tokens_mask = self._calc_greenlist_mask(scores, greenlist_ids)\n",
    "        scores[green_tokens_mask] = scores[green_tokens_mask] + self.delta\n",
    "        return scores\n",
    "    \n",
    "class WatermarkDetector(WatermarkBase):\n",
    "    def __init__( self, *args, **kwargs, ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.min_prefix_len = 1\n",
    "     \n",
    "    def score_sequence( self, input_ids: torch.Tensor):\n",
    "        num_tokens_scored = len(input_ids) - self.min_prefix_len\n",
    "        green_token_count = 0\n",
    "        green_token_mask = []\n",
    "        for idx in range(self.min_prefix_len, len(input_ids)):\n",
    "            curr_token = input_ids[idx]\n",
    "            greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\n",
    "            if curr_token in greenlist_ids:\n",
    "                green_token_count += 1\n",
    "                green_token_mask.append(True)\n",
    "            else:\n",
    "                green_token_mask.append(False)\n",
    "\n",
    "        score_dict = dict(\n",
    "            num_tokens_scored=num_tokens_scored,\n",
    "            num_green_tokens=green_token_count,\n",
    "            green_fraction=(green_token_count / num_tokens_scored),\n",
    "            green_token_mask=green_token_mask\n",
    "        )\n",
    "\n",
    "        return score_dict\n",
    "    \n",
    "watermarker = LLMWatermarker(vocab = tokenizer.get_vocab())\n",
    "detector = WatermarkDetector(vocab = tokenizer.get_vocab())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora ringenero dallo spesso prompt ma aggiungendo il watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: per simulare\n",
      "generated text: per simulare un ruolo importante, i modelli di questo tipo di produzione che lo hanno rimpatricando come la maggior parte dei siti di comunicazione, di cui si\n",
      "[549, 11705, 607, 296, 2041, 1928, 11, 281, 3770, 271, 661, 1276, 271, 1636, 315, 448, 799, 329, 1484, 427, 2343, 452, 298, 1490, 567, 424, 6135, 271, 4526, 11, 271, 554, 352]\n"
     ]
    }
   ],
   "source": [
    "xw = encode_input(initial_prompt)\n",
    "for _ in range(max_new_tokens):\n",
    "    logits = get_logits(model, xw, temperature, top_k)\n",
    "    logits = watermarker.watermark(xw, logits)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    xw = torch.cat((xw, idx_next), dim=1)\n",
    "\n",
    "\n",
    "print(\"input text:\", initial_prompt)\n",
    "print(\"generated text:\", decode(xw[0].tolist()))\n",
    "print(xw[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green fraction without watermark: 0.44\n",
      "green fraction with watermark: 0.78\n"
     ]
    }
   ],
   "source": [
    "tokenized_text_not_watermarked = x_not_watermarked[0].tolist()\n",
    "tokenized_text_with_watermark = xw[0].tolist()\n",
    "not_watermarked_stats = detector.score_sequence(tokenized_text_not_watermarked)\n",
    "completion_stats =  detector.score_sequence(tokenized_text_with_watermark)\n",
    "\n",
    "not_wm_green_fraction = not_watermarked_stats['green_fraction']\n",
    "wm_green_fraction = completion_stats['green_fraction']\n",
    "\n",
    "print(f\"green fraction without watermark: {not_wm_green_fraction:.2f}\")\n",
    "print(f\"green fraction with watermark: {wm_green_fraction:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pro?\n",
    "- Veloce\n",
    "- Preserva varieta\n",
    "- Non segreto\n",
    "- Non serve pubblicare i pesi del modello\n",
    "- \"Resistente\"\n",
    "\n",
    "### Contro?\n",
    "- Richiede accesso alle probabilità calcolate dal LLM\n",
    "    - il modello deve essere proprietario\n",
    "    - non può essere usato post-generazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
